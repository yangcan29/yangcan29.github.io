# 概念
- 学习系统没有像很多其它形式的机器学习方法一样被告知应该做出什么行为
- 必须在尝试了之后才能发现哪些行为会导致奖励的最大化
- 当前的行为可能不仅仅会影响即时奖励，还会影响下一步的奖励以及后续的所有奖励
## 相关术语
- 智能体(agent)：进行学习的主体
- 状态(state)：周围的环境
- 行为(action)：智能体进行的下一步活动或动作
- 奖励(reward)：产生的不同结果，操作的奖励或惩罚
- 策略(policy)：实现目标的动作组合或措施

## 目标
- 选择一系列行动来最大化未来的奖励

每一个动作(action)都能影响代理将来的状态(state)，通过一个标量的奖励(reward)信号来衡量成功

## 状态
- 经验是观察，行为，奖励的序列
```math
o(1)，r(1)，a(1)，...，a(t-1)，o(t)，r(t)
```
- 状态是经验的总和
```math
s(t) = f(o(1)，r(1)，a(1)，...，a(t-1)，o(t)，r(t))
```
# 马尔科夫决策
## 要求
1. 能够检测到理想的状态
2. 可以多次尝试
3. 系统的下个状态只与当前状态信息有关，而与更早之前的状态无关，在决策过程中还和当前采取的动作有关
## 构成
马尔科夫决策过程由5个元素构成：
- S：表示状态集（states）
- A：表示一组动作（actions）
- P：表示状态转移概率，Psa表示在当前s（s属于S状态）下，经过a（a属于A）作用后，会转移到其他状态的概率分布情况
- R：奖励函数（reward function），表示agent采取某个动作后的即时奖励
- Y：折扣系数，意味着当下的reward比未来反馈的reward更重要

**状态价值函数v(s)**：t时刻的状态s能够获得未来回报的期望。

价值函数用来衡量某一状态或状态-动作对的优劣价，累计奖励的期望。

**最有价值函数**：所有策略下的最优累计奖励期望。

**策略**：已知状态下，可能产生动作的概率分布。

**Bellman方程**：当前状态的价值和下一步的价值以及当前的奖励（reward）有关。

价值函数分解为**当前的奖励**和**下一步的价值**两部分。

因为动作空间A，状态空间S均为有限集合，所以我们可以用求和来计算期望。

# Q learning
Q-Learning算法下的智能体agent，不知道整体的环境，知道当前状态下可以选择哪些动作。

通常，我们需要构建一个即时奖励矩阵R，用于表示从状态s到下一个状态s’的动作奖励值。

由即时奖励矩阵R计算得出指导agent行动的Q矩阵，然后不断扩充更新Q矩阵，最后使Q矩阵收敛。

核心公式：
```math
Q(s,a) = R(s,a) +gamma*[max(s',a')]
```
其中s,a表示当前的状态和行为，s'和a'表示s的下一个状态及行为，学习参数gamma为满足0<=gamma<=1的常数。



Step 1 ：给定参数gamma和reward矩阵R

Step 2 ：令Q:=0

Step 3 ：For each episode：

3.1 随机选择一个初始状态s.

3.2 若未达到目标状态，则执行以下几步

(1)在当前状态s的所有可能行为中选取一个行为a

(2)利用选定的行为a，得到下一个状态s'

(3)按照公式计算Q(s,a)

(4)令s:=s'

